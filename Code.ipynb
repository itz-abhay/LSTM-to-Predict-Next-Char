{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Getting the Data from Site**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3\n",
    "\n",
    "# Create a PoolManager instance\n",
    "http = urllib3.PoolManager()\n",
    "\n",
    "# Specify the URL you want to request\n",
    "url = \"https://www.gutenberg.org/cache/epub/11/pg11-images.html\" #link to the Book\n",
    "url_2 = \"https://www.gutenberg.org/cache/epub/11/pg11.txt\" #Link of plain text of the above mentioned book\n",
    "\n",
    "# Send an HTTP GET request\n",
    "Data = http.request(\"GET\", url_2)\n",
    "\n",
    "# Read and print the response content\n",
    "Data = Data.data.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  167711\n",
      "Total Vocab:  66\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load ascii text and covert to lowercase\n",
    "raw_text = Data\n",
    "raw_text = raw_text.lower()\n",
    "\n",
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare the dataset of input to output pairs encoded as integers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  167611\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([167611, 100, 1]) torch.Size([167611])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = torch.tensor(dataX, dtype=torch.float32).reshape(n_patterns, seq_length, 1)\n",
    "X = X / float(n_vocab)\n",
    "y = torch.tensor(dataY)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(256, n_vocab)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Cross-entropy: 479210.7812\n",
      "Epoch 1: Cross-entropy: 457004.2500\n",
      "Epoch 2: Cross-entropy: 443024.7188\n",
      "Epoch 3: Cross-entropy: 431901.3125\n",
      "Epoch 4: Cross-entropy: 422259.3750\n",
      "Epoch 5: Cross-entropy: 414639.7500\n",
      "Epoch 6: Cross-entropy: 408252.2812\n",
      "Epoch 7: Cross-entropy: 397344.5000\n",
      "Epoch 8: Cross-entropy: 391695.0938\n",
      "Epoch 9: Cross-entropy: 383857.7188\n",
      "Epoch 10: Cross-entropy: 377220.1562\n",
      "Epoch 11: Cross-entropy: 370299.9062\n",
      "Epoch 12: Cross-entropy: 364141.9062\n",
      "Epoch 13: Cross-entropy: 357340.0625\n",
      "Epoch 14: Cross-entropy: 353803.5938\n",
      "Epoch 15: Cross-entropy: 348545.3750\n",
      "Epoch 16: Cross-entropy: 340985.1562\n",
      "Epoch 17: Cross-entropy: 338929.0000\n",
      "Epoch 18: Cross-entropy: 332969.2500\n",
      "Epoch 19: Cross-entropy: 328854.5625\n",
      "Epoch 20: Cross-entropy: 324042.2188\n",
      "Epoch 21: Cross-entropy: 320053.8125\n",
      "Epoch 22: Cross-entropy: 315985.5625\n",
      "Epoch 23: Cross-entropy: 312282.5000\n",
      "Epoch 24: Cross-entropy: 307640.5000\n",
      "Epoch 25: Cross-entropy: 303933.3125\n",
      "Epoch 26: Cross-entropy: 302122.6562\n",
      "Epoch 27: Cross-entropy: 300940.9062\n",
      "Epoch 28: Cross-entropy: 297266.7188\n",
      "Epoch 29: Cross-entropy: 294795.4688\n",
      "Epoch 30: Cross-entropy: 290832.5625\n",
      "Epoch 31: Cross-entropy: 288843.0312\n",
      "Epoch 32: Cross-entropy: 285066.2500\n",
      "Epoch 33: Cross-entropy: 285701.9062\n",
      "Epoch 34: Cross-entropy: 281882.0625\n",
      "Epoch 35: Cross-entropy: 278282.9375\n",
      "Epoch 36: Cross-entropy: 277567.1875\n",
      "Epoch 37: Cross-entropy: 273591.8750\n",
      "Epoch 38: Cross-entropy: 272798.1875\n",
      "Epoch 39: Cross-entropy: 270105.8125\n",
      "Epoch 40: Cross-entropy: 268668.9375\n",
      "Epoch 41: Cross-entropy: 287932.3125\n",
      "Epoch 42: Cross-entropy: 264002.8438\n",
      "Epoch 43: Cross-entropy: 261450.7656\n",
      "Epoch 44: Cross-entropy: 262899.4688\n",
      "Epoch 45: Cross-entropy: 262457.1875\n",
      "Epoch 46: Cross-entropy: 261823.3906\n",
      "Epoch 47: Cross-entropy: 265244.7812\n",
      "Epoch 48: Cross-entropy: 264833.9375\n",
      "Epoch 49: Cross-entropy: 259373.4219\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "batch_size = 128\n",
    "model = CharModel()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "loader = data.DataLoader(data.TensorDataset(X, y), shuffle=True, batch_size=batch_size)\n",
    "\n",
    "best_model = None\n",
    "best_loss = np.inf\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    for X_batch, y_batch in loader:\n",
    "        y_pred = model(X_batch)\n",
    "        loss = loss_fn(y_pred, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in loader:\n",
    "            y_pred = model(X_batch)\n",
    "            loss += loss_fn(y_pred, y_batch)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "            best_model = model.state_dict()\n",
    "        print(\"Epoch %d: Cross-entropy: %.4f\" % (epoch, loss))\n",
    "\n",
    "torch.save([best_model, char_to_int], \"single-char.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "prompt = raw_text[start:start+seq_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: \"—“and perhaps you were never even introduced to a \"\n",
      "\n",
      "erere if lhrtle thitg thie she\n",
      "toeee tai she was not in the\n",
      "tooe of the tire, “ho wou make\n",
      "toenting the darten of the\n",
      "court woth the cart, and the dound\n",
      "to the sooe, and thnn allie the\n",
      "aourd tet an to coes and thing\n",
      "an she could, and sai an the\n",
      "cal no tie was ootning to the \n",
      "tooe of the sare oate and toe\n",
      "prmee of the sime and the war.\n",
      "and sande toel a little bool and all\n",
      "to she shate oaat the whsle so\n",
      "the foomt on the tas an the could\n",
      "to the saade to the sheee toine an\n",
      "the foumouse so he a det fntt\n",
      "co ant the saadit sote the soeee\n",
      "an      aed tie project gutenbered th hes bednd\n",
      "thet she was soiniigg to the\n",
      "tooe of the sooe, “in aset_ \n",
      "io whe winte tase toin thet\n",
      "iire aod toned an the coum\n",
      "teat the darte sait, “ho mo toene\n",
      "io whu hid toe bare in the\n",
      "courd toted the carerell an the\n",
      "poosensn a aievert coun and tonne\n",
      "an                                                                                                                                                    \n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "best_model, char_to_int = torch.load(\"single-char.pth\")\n",
    "n_vocab = len(char_to_int)\n",
    "int_to_char = dict((i, c) for c, i in char_to_int.items())\n",
    "\n",
    "# reload the model\n",
    "class CharModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=1, hidden_size=256, num_layers=1, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear = nn.Linear(256, n_vocab)\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        # take only the last output\n",
    "        x = x[:, -1, :]\n",
    "        # produce output\n",
    "        x = self.linear(self.dropout(x))\n",
    "        return x\n",
    "\n",
    "model = CharModel()\n",
    "model.load_state_dict(best_model)\n",
    "\n",
    "# randomly generate a prompt\n",
    "seq_length = 50\n",
    "raw_text = Data.lower()\n",
    "start = np.random.randint(0, len(raw_text)-seq_length)\n",
    "prompt = raw_text[start:start+seq_length]\n",
    "pattern = [char_to_int[c] for c in prompt]\n",
    "\n",
    "model.eval()\n",
    "print('Prompt: \"%s\"' % prompt)\n",
    "with torch.no_grad():\n",
    "    for i in range(1000):\n",
    "        # format input array of int into PyTorch tensor\n",
    "        x = np.reshape(pattern, (1, len(pattern), 1)) / float(n_vocab)\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        # generate logits as output from the model\n",
    "        prediction = model(x)\n",
    "        # convert logits into one character\n",
    "        index = int(prediction.argmax())\n",
    "        result = int_to_char[index]\n",
    "        print(result, end=\"\")\n",
    "        # append the new character into the prompt for the next iteration\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:]\n",
    "print()\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iiscWorkspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
